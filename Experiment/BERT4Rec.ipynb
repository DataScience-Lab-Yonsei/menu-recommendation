{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modules & Hyper-parameters"
      ],
      "metadata": {
        "id": "iQjtkpOT7rqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM_cwpX27bL9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model setting\n",
        "max_len = 70\n",
        "hidden_units = 50\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "dropout_rate=0.5\n",
        "num_workers = 1\n",
        "device = 'cuda' \n",
        "\n",
        "# training setting\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 200\n",
        "mask_prob = 0.15 # for cloze task"
      ],
      "metadata": {
        "id": "YLLDOnIi719X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/[22-2]DSL_Modeling/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxVS5Snl8FDF",
        "outputId": "84f2ec70-f617-4234-ac16-5508d32ea6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/[22-2]DSL_Modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리"
      ],
      "metadata": {
        "id": "0v6nlHXN75JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('menu_final.csv')"
      ],
      "metadata": {
        "id": "pPrFBkoNXzLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R1oma7e9YcQA",
        "outputId": "df56f0a6-e6fe-4546-f606-33275055519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  SessionID   Menu  MenuID  timestamp\n",
              "0           0          0     찐빵     0.0          1\n",
              "1           1          0  오징어찌개     1.0          2\n",
              "2           2          0    육개장     2.0          3\n",
              "3           3          0  단호박샌드     3.0          4\n",
              "4           4          0   김치찌개     4.0          5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8612416-8b13-437d-a24c-1233ca16ea6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SessionID</th>\n",
              "      <th>Menu</th>\n",
              "      <th>MenuID</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>찐빵</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>오징어찌개</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>육개장</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>단호박샌드</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>김치찌개</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8612416-8b13-437d-a24c-1233ca16ea6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8612416-8b13-437d-a24c-1233ca16ea6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8612416-8b13-437d-a24c-1233ca16ea6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['SessionID','Menu','timestamp']]"
      ],
      "metadata": {
        "id": "YytOFSpqYzfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_list = ['user','item','time']\n",
        "df.columns = col_list\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Z14j5-PAZEyA",
        "outputId": "35d382f6-a024-476d-baaa-b742e17c1fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user   item  time\n",
              "0     0     찐빵     1\n",
              "1     0  오징어찌개     2\n",
              "2     0    육개장     3\n",
              "3     0  단호박샌드     4\n",
              "4     0   김치찌개     5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b16ed75a-5b76-4792-a35e-d683862aefcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>찐빵</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>오징어찌개</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>육개장</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>단호박샌드</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>김치찌개</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b16ed75a-5b76-4792-a35e-d683862aefcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b16ed75a-5b76-4792-a35e-d683862aefcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b16ed75a-5b76-4792-a35e-d683862aefcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = 'dacon_menu_hr3.csv'\n",
        "# df = pd.read_csv(data_path)\n",
        "\n",
        "item_ids = df['item'].unique()\n",
        "user_ids = df['user'].unique()\n",
        "num_item, num_user = len(item_ids), len(user_ids)\n",
        "num_batch = num_user // batch_size\n",
        "\n",
        "# user, item indexing\n",
        "item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item), num_item+1: mask idx\n",
        "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
        "\n",
        "# dataframe indexing\n",
        "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
        "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
        "df.sort_values(['user_idx', 'time'], inplace=True)\n",
        "check_user = defaultdict()\n",
        "check_item = defaultdict()\n",
        "for u, i in zip(df['user_idx'], df['user']):\n",
        "    check_user[u] = i\n",
        "for u, i in zip(df['item_idx'], df['item']):\n",
        "    check_item[u] = i\n",
        "\n",
        "del df['item'], df['user'] \n",
        "\n",
        "# train set, valid set 생성\n",
        "users = defaultdict(list) \n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
        "    users[u].append(i)\n",
        "\n",
        "for user in users:\n",
        "    user_train[user] = users[user][:-1]\n",
        "    user_valid[user] = [users[user][-1]]\n",
        "\n",
        "print(f'num users: {num_user}, num items: {num_item}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51o4ckcd74oV",
        "outputId": "035f4b86-4f6c-46d9-e930-4b940c04fb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num users: 4583, num items: 2399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_train, num_user, num_item, max_len, mask_prob):\n",
        "        self.user_train = user_train\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.max_len = max_len\n",
        "        self.mask_prob = mask_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, user): \n",
        "        # iterator를 구동할 때 사용\n",
        "        seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in seq:\n",
        "            prob = np.random.random() \n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "\n",
        "                # BERT 학습\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    tokens.append(self.num_item + 1)  # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
        "                elif prob < 0.9:\n",
        "                    tokens.append(np.random.randint(1, self.num_item+1))  # item random sampling\n",
        "                else:\n",
        "                    tokens.append(s)\n",
        "                labels.append(s)  # 학습에 사용\n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0)  # 학습에 사용 X, trivial\n",
        "        tokens = tokens[-self.max_len:]\n",
        "        labels = labels[-self.max_len:]\n",
        "        mask_len = self.max_len - len(tokens)\n",
        "\n",
        "        # zero padding\n",
        "        tokens = [0] * mask_len + tokens\n",
        "        labels = [0] * mask_len + labels\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)"
      ],
      "metadata": {
        "id": "W4D87hSw-5SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "pisnQI6V-7rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # head의 수\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # scaled dot product attention module을 사용하여 attention 계산\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "        \n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) \n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합침\n",
        "        output = output.transpose(1, 2).contiguous() \n",
        "        output = output.view(batch_size, seqlen, -1)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "        return output, attn_dist\n",
        "    \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        \n",
        "        self.W_1 = nn.Linear(hidden_units, 4 * hidden_units) \n",
        "        self.W_2 = nn.Linear(4 * hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.gelu(self.dropout(self.W_1(x)))) # activation: relu -> gelu\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output\n",
        "    \n",
        "class BERT4RecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(BERT4RecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist"
      ],
      "metadata": {
        "id": "XIGQmnh1-8wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT4Rec"
      ],
      "metadata": {
        "id": "A1iew-JH-_t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
        "        super(BERT4Rec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers \n",
        "        self.device = device\n",
        "        \n",
        "        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) \n",
        "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "        self.out = nn.Linear(hidden_units, num_item + 1) \n",
        "        \n",
        "    def forward(self, log_seqs):\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
        "\n",
        "        mask = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask)\n",
        "        out = self.out(seqs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lw8icJdP_CNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "lqecLOfB_Gpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT4Rec(num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "seq_dataset = SeqDataset(user_train, num_user, num_item, max_len, mask_prob)\n",
        "data_loader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, pin_memory=True) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "yWRMInNB_H2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    tbar = tqdm(data_loader)\n",
        "    for step, (log_seqs, labels) in enumerate(tbar):\n",
        "        logits = model(log_seqs)\n",
        "        \n",
        "        # size matching\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        labels = labels.view(-1).to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Step: {step:3d}| Train loss: {loss:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NyXgn_K_Qw3",
        "outputId": "d23ae2a0-c834-4cf7-9610-18660cd94d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Step:  35| Train loss: 7.24401: 100%|██████████| 36/36 [00:03<00:00, 10.18it/s]\n",
            "Epoch:   2| Step:  35| Train loss: 6.93244: 100%|██████████| 36/36 [00:00<00:00, 51.80it/s]\n",
            "Epoch:   3| Step:  35| Train loss: 6.87757: 100%|██████████| 36/36 [00:00<00:00, 51.71it/s]\n",
            "Epoch:   4| Step:  35| Train loss: 6.88382: 100%|██████████| 36/36 [00:00<00:00, 51.81it/s]\n",
            "Epoch:   5| Step:  35| Train loss: 6.85682: 100%|██████████| 36/36 [00:00<00:00, 52.40it/s]\n",
            "Epoch:   6| Step:  35| Train loss: 6.96996: 100%|██████████| 36/36 [00:00<00:00, 53.50it/s]\n",
            "Epoch:   7| Step:  35| Train loss: 6.55503: 100%|██████████| 36/36 [00:00<00:00, 52.26it/s]\n",
            "Epoch:   8| Step:  35| Train loss: 6.75306: 100%|██████████| 36/36 [00:00<00:00, 53.40it/s]\n",
            "Epoch:   9| Step:  35| Train loss: 6.50607: 100%|██████████| 36/36 [00:00<00:00, 54.04it/s]\n",
            "Epoch:  10| Step:  35| Train loss: 6.44438: 100%|██████████| 36/36 [00:00<00:00, 53.74it/s]\n",
            "Epoch:  11| Step:  35| Train loss: 6.32012: 100%|██████████| 36/36 [00:00<00:00, 52.19it/s]\n",
            "Epoch:  12| Step:  35| Train loss: 6.35012: 100%|██████████| 36/36 [00:00<00:00, 52.84it/s]\n",
            "Epoch:  13| Step:  35| Train loss: 6.31321: 100%|██████████| 36/36 [00:00<00:00, 51.72it/s]\n",
            "Epoch:  14| Step:  35| Train loss: 6.23636: 100%|██████████| 36/36 [00:00<00:00, 52.72it/s]\n",
            "Epoch:  15| Step:  35| Train loss: 6.23171: 100%|██████████| 36/36 [00:00<00:00, 52.66it/s]\n",
            "Epoch:  16| Step:  35| Train loss: 6.32168: 100%|██████████| 36/36 [00:00<00:00, 53.16it/s]\n",
            "Epoch:  17| Step:  35| Train loss: 6.15834: 100%|██████████| 36/36 [00:00<00:00, 52.63it/s]\n",
            "Epoch:  18| Step:  35| Train loss: 6.01238: 100%|██████████| 36/36 [00:00<00:00, 51.78it/s]\n",
            "Epoch:  19| Step:  35| Train loss: 5.83203: 100%|██████████| 36/36 [00:00<00:00, 52.34it/s]\n",
            "Epoch:  20| Step:  35| Train loss: 6.13069: 100%|██████████| 36/36 [00:00<00:00, 53.08it/s]\n",
            "Epoch:  21| Step:  35| Train loss: 5.91301: 100%|██████████| 36/36 [00:00<00:00, 52.63it/s]\n",
            "Epoch:  22| Step:  35| Train loss: 6.02764: 100%|██████████| 36/36 [00:00<00:00, 52.44it/s]\n",
            "Epoch:  23| Step:  35| Train loss: 5.96888: 100%|██████████| 36/36 [00:00<00:00, 52.75it/s]\n",
            "Epoch:  24| Step:  35| Train loss: 6.00935: 100%|██████████| 36/36 [00:00<00:00, 52.93it/s]\n",
            "Epoch:  25| Step:  35| Train loss: 5.89983: 100%|██████████| 36/36 [00:00<00:00, 53.40it/s]\n",
            "Epoch:  26| Step:  35| Train loss: 5.80692: 100%|██████████| 36/36 [00:00<00:00, 54.04it/s]\n",
            "Epoch:  27| Step:  35| Train loss: 5.86804: 100%|██████████| 36/36 [00:00<00:00, 52.17it/s]\n",
            "Epoch:  28| Step:  35| Train loss: 5.74793: 100%|██████████| 36/36 [00:00<00:00, 53.09it/s]\n",
            "Epoch:  29| Step:  35| Train loss: 6.06648: 100%|██████████| 36/36 [00:00<00:00, 53.68it/s]\n",
            "Epoch:  30| Step:  35| Train loss: 5.80618: 100%|██████████| 36/36 [00:00<00:00, 53.58it/s]\n",
            "Epoch:  31| Step:  35| Train loss: 5.59672: 100%|██████████| 36/36 [00:00<00:00, 51.59it/s]\n",
            "Epoch:  32| Step:  35| Train loss: 5.67286: 100%|██████████| 36/36 [00:00<00:00, 53.05it/s]\n",
            "Epoch:  33| Step:  35| Train loss: 5.59422: 100%|██████████| 36/36 [00:00<00:00, 51.79it/s]\n",
            "Epoch:  34| Step:  35| Train loss: 5.57865: 100%|██████████| 36/36 [00:00<00:00, 53.13it/s]\n",
            "Epoch:  35| Step:  35| Train loss: 5.55928: 100%|██████████| 36/36 [00:00<00:00, 53.09it/s]\n",
            "Epoch:  36| Step:  35| Train loss: 5.48227: 100%|██████████| 36/36 [00:00<00:00, 52.97it/s]\n",
            "Epoch:  37| Step:  35| Train loss: 5.48042: 100%|██████████| 36/36 [00:00<00:00, 52.36it/s]\n",
            "Epoch:  38| Step:  35| Train loss: 5.28632: 100%|██████████| 36/36 [00:00<00:00, 53.70it/s]\n",
            "Epoch:  39| Step:  35| Train loss: 5.41679: 100%|██████████| 36/36 [00:00<00:00, 53.01it/s]\n",
            "Epoch:  40| Step:  35| Train loss: 5.27221: 100%|██████████| 36/36 [00:00<00:00, 52.77it/s]\n",
            "Epoch:  41| Step:  35| Train loss: 5.23221: 100%|██████████| 36/36 [00:00<00:00, 53.56it/s]\n",
            "Epoch:  42| Step:  35| Train loss: 5.16318: 100%|██████████| 36/36 [00:00<00:00, 51.19it/s]\n",
            "Epoch:  43| Step:  35| Train loss: 5.40506: 100%|██████████| 36/36 [00:00<00:00, 52.40it/s]\n",
            "Epoch:  44| Step:  35| Train loss: 5.33394: 100%|██████████| 36/36 [00:00<00:00, 54.18it/s]\n",
            "Epoch:  45| Step:  35| Train loss: 5.26816: 100%|██████████| 36/36 [00:00<00:00, 53.96it/s]\n",
            "Epoch:  46| Step:  35| Train loss: 5.25367: 100%|██████████| 36/36 [00:00<00:00, 53.88it/s]\n",
            "Epoch:  47| Step:  35| Train loss: 5.45509: 100%|██████████| 36/36 [00:00<00:00, 53.64it/s]\n",
            "Epoch:  48| Step:  35| Train loss: 5.09287: 100%|██████████| 36/36 [00:00<00:00, 52.61it/s]\n",
            "Epoch:  49| Step:  35| Train loss: 5.22067: 100%|██████████| 36/36 [00:00<00:00, 52.52it/s]\n",
            "Epoch:  50| Step:  35| Train loss: 5.24553: 100%|██████████| 36/36 [00:00<00:00, 53.74it/s]\n",
            "Epoch:  51| Step:  35| Train loss: 5.27504: 100%|██████████| 36/36 [00:00<00:00, 53.43it/s]\n",
            "Epoch:  52| Step:  35| Train loss: 5.17286: 100%|██████████| 36/36 [00:00<00:00, 52.38it/s]\n",
            "Epoch:  53| Step:  35| Train loss: 5.25808: 100%|██████████| 36/36 [00:00<00:00, 53.89it/s]\n",
            "Epoch:  54| Step:  35| Train loss: 5.23361: 100%|██████████| 36/36 [00:00<00:00, 52.62it/s]\n",
            "Epoch:  55| Step:  35| Train loss: 5.12152: 100%|██████████| 36/36 [00:00<00:00, 54.05it/s]\n",
            "Epoch:  56| Step:  35| Train loss: 5.05854: 100%|██████████| 36/36 [00:00<00:00, 53.41it/s]\n",
            "Epoch:  57| Step:  35| Train loss: 4.98098: 100%|██████████| 36/36 [00:00<00:00, 52.23it/s]\n",
            "Epoch:  58| Step:  35| Train loss: 5.08113: 100%|██████████| 36/36 [00:00<00:00, 53.48it/s]\n",
            "Epoch:  59| Step:  35| Train loss: 5.18441: 100%|██████████| 36/36 [00:00<00:00, 52.85it/s]\n",
            "Epoch:  60| Step:  35| Train loss: 5.13383: 100%|██████████| 36/36 [00:00<00:00, 53.80it/s]\n",
            "Epoch:  61| Step:  35| Train loss: 5.04643: 100%|██████████| 36/36 [00:00<00:00, 52.85it/s]\n",
            "Epoch:  62| Step:  35| Train loss: 5.01058: 100%|██████████| 36/36 [00:00<00:00, 53.13it/s]\n",
            "Epoch:  63| Step:  35| Train loss: 5.18384: 100%|██████████| 36/36 [00:00<00:00, 52.93it/s]\n",
            "Epoch:  64| Step:  35| Train loss: 4.93407: 100%|██████████| 36/36 [00:00<00:00, 52.08it/s]\n",
            "Epoch:  65| Step:  35| Train loss: 4.99922: 100%|██████████| 36/36 [00:00<00:00, 53.16it/s]\n",
            "Epoch:  66| Step:  35| Train loss: 4.95730: 100%|██████████| 36/36 [00:00<00:00, 53.19it/s]\n",
            "Epoch:  67| Step:  35| Train loss: 4.99181: 100%|██████████| 36/36 [00:00<00:00, 52.56it/s]\n",
            "Epoch:  68| Step:  35| Train loss: 4.92622: 100%|██████████| 36/36 [00:00<00:00, 52.42it/s]\n",
            "Epoch:  69| Step:  35| Train loss: 4.98333: 100%|██████████| 36/36 [00:00<00:00, 52.70it/s]\n",
            "Epoch:  70| Step:  35| Train loss: 5.08477: 100%|██████████| 36/36 [00:00<00:00, 52.55it/s]\n",
            "Epoch:  71| Step:  35| Train loss: 4.99613: 100%|██████████| 36/36 [00:00<00:00, 53.64it/s]\n",
            "Epoch:  72| Step:  35| Train loss: 5.15756: 100%|██████████| 36/36 [00:00<00:00, 52.25it/s]\n",
            "Epoch:  73| Step:  35| Train loss: 4.89548: 100%|██████████| 36/36 [00:00<00:00, 53.04it/s]\n",
            "Epoch:  74| Step:  35| Train loss: 5.04033: 100%|██████████| 36/36 [00:00<00:00, 53.30it/s]\n",
            "Epoch:  75| Step:  35| Train loss: 4.94942: 100%|██████████| 36/36 [00:00<00:00, 52.75it/s]\n",
            "Epoch:  76| Step:  35| Train loss: 4.87692: 100%|██████████| 36/36 [00:00<00:00, 53.52it/s]\n",
            "Epoch:  77| Step:  35| Train loss: 4.88432: 100%|██████████| 36/36 [00:00<00:00, 53.15it/s]\n",
            "Epoch:  78| Step:  35| Train loss: 4.92533: 100%|██████████| 36/36 [00:00<00:00, 52.24it/s]\n",
            "Epoch:  79| Step:  35| Train loss: 4.97320: 100%|██████████| 36/36 [00:00<00:00, 52.81it/s]\n",
            "Epoch:  80| Step:  35| Train loss: 4.91142: 100%|██████████| 36/36 [00:00<00:00, 53.37it/s]\n",
            "Epoch:  81| Step:  35| Train loss: 4.91327: 100%|██████████| 36/36 [00:00<00:00, 52.23it/s]\n",
            "Epoch:  82| Step:  35| Train loss: 4.83634: 100%|██████████| 36/36 [00:00<00:00, 50.56it/s]\n",
            "Epoch:  83| Step:  35| Train loss: 4.96188: 100%|██████████| 36/36 [00:00<00:00, 51.31it/s]\n",
            "Epoch:  84| Step:  35| Train loss: 4.93641: 100%|██████████| 36/36 [00:00<00:00, 50.41it/s]\n",
            "Epoch:  85| Step:  35| Train loss: 4.92801: 100%|██████████| 36/36 [00:00<00:00, 51.67it/s]\n",
            "Epoch:  86| Step:  35| Train loss: 4.78948: 100%|██████████| 36/36 [00:00<00:00, 53.84it/s]\n",
            "Epoch:  87| Step:  35| Train loss: 4.85659: 100%|██████████| 36/36 [00:00<00:00, 51.89it/s]\n",
            "Epoch:  88| Step:  35| Train loss: 4.69646: 100%|██████████| 36/36 [00:00<00:00, 53.35it/s]\n",
            "Epoch:  89| Step:  35| Train loss: 4.67643: 100%|██████████| 36/36 [00:00<00:00, 53.12it/s]\n",
            "Epoch:  90| Step:  35| Train loss: 4.76756: 100%|██████████| 36/36 [00:00<00:00, 53.04it/s]\n",
            "Epoch:  91| Step:  35| Train loss: 4.75728: 100%|██████████| 36/36 [00:00<00:00, 52.78it/s]\n",
            "Epoch:  92| Step:  35| Train loss: 4.75146: 100%|██████████| 36/36 [00:00<00:00, 53.54it/s]\n",
            "Epoch:  93| Step:  35| Train loss: 4.93170: 100%|██████████| 36/36 [00:00<00:00, 51.08it/s]\n",
            "Epoch:  94| Step:  35| Train loss: 4.93135: 100%|██████████| 36/36 [00:00<00:00, 52.19it/s]\n",
            "Epoch:  95| Step:  35| Train loss: 5.00795: 100%|██████████| 36/36 [00:00<00:00, 52.91it/s]\n",
            "Epoch:  96| Step:  35| Train loss: 4.75214: 100%|██████████| 36/36 [00:00<00:00, 53.39it/s]\n",
            "Epoch:  97| Step:  35| Train loss: 4.84894: 100%|██████████| 36/36 [00:00<00:00, 53.09it/s]\n",
            "Epoch:  98| Step:  35| Train loss: 4.85623: 100%|██████████| 36/36 [00:00<00:00, 53.33it/s]\n",
            "Epoch:  99| Step:  35| Train loss: 4.80210: 100%|██████████| 36/36 [00:00<00:00, 53.10it/s]\n",
            "Epoch: 100| Step:  35| Train loss: 4.64396: 100%|██████████| 36/36 [00:00<00:00, 52.86it/s]\n",
            "Epoch: 101| Step:  35| Train loss: 4.76251: 100%|██████████| 36/36 [00:00<00:00, 52.24it/s]\n",
            "Epoch: 102| Step:  35| Train loss: 4.83976: 100%|██████████| 36/36 [00:00<00:00, 53.07it/s]\n",
            "Epoch: 103| Step:  35| Train loss: 4.72433: 100%|██████████| 36/36 [00:00<00:00, 53.07it/s]\n",
            "Epoch: 104| Step:  35| Train loss: 4.87554: 100%|██████████| 36/36 [00:00<00:00, 52.64it/s]\n",
            "Epoch: 105| Step:  35| Train loss: 4.68016: 100%|██████████| 36/36 [00:00<00:00, 52.77it/s]\n",
            "Epoch: 106| Step:  35| Train loss: 4.77442: 100%|██████████| 36/36 [00:00<00:00, 52.83it/s]\n",
            "Epoch: 107| Step:  35| Train loss: 4.82444: 100%|██████████| 36/36 [00:00<00:00, 51.74it/s]\n",
            "Epoch: 108| Step:  35| Train loss: 4.76000: 100%|██████████| 36/36 [00:00<00:00, 53.37it/s]\n",
            "Epoch: 109| Step:  35| Train loss: 4.69485: 100%|██████████| 36/36 [00:00<00:00, 52.90it/s]\n",
            "Epoch: 110| Step:  35| Train loss: 4.66326: 100%|██████████| 36/36 [00:00<00:00, 52.26it/s]\n",
            "Epoch: 111| Step:  35| Train loss: 4.67979: 100%|██████████| 36/36 [00:00<00:00, 52.86it/s]\n",
            "Epoch: 112| Step:  35| Train loss: 4.78271: 100%|██████████| 36/36 [00:00<00:00, 52.70it/s]\n",
            "Epoch: 113| Step:  35| Train loss: 4.58615: 100%|██████████| 36/36 [00:00<00:00, 53.07it/s]\n",
            "Epoch: 114| Step:  35| Train loss: 4.72927: 100%|██████████| 36/36 [00:00<00:00, 52.72it/s]\n",
            "Epoch: 115| Step:  35| Train loss: 4.74901: 100%|██████████| 36/36 [00:00<00:00, 53.51it/s]\n",
            "Epoch: 116| Step:  35| Train loss: 4.73790: 100%|██████████| 36/36 [00:00<00:00, 52.10it/s]\n",
            "Epoch: 117| Step:  35| Train loss: 4.79643: 100%|██████████| 36/36 [00:00<00:00, 52.64it/s]\n",
            "Epoch: 118| Step:  35| Train loss: 4.66812: 100%|██████████| 36/36 [00:00<00:00, 53.19it/s]\n",
            "Epoch: 119| Step:  35| Train loss: 4.74716: 100%|██████████| 36/36 [00:00<00:00, 51.56it/s]\n",
            "Epoch: 120| Step:  35| Train loss: 4.67965: 100%|██████████| 36/36 [00:00<00:00, 54.01it/s]\n",
            "Epoch: 121| Step:  35| Train loss: 4.62139: 100%|██████████| 36/36 [00:00<00:00, 52.82it/s]\n",
            "Epoch: 122| Step:  35| Train loss: 4.77820: 100%|██████████| 36/36 [00:00<00:00, 51.60it/s]\n",
            "Epoch: 123| Step:  35| Train loss: 4.71481: 100%|██████████| 36/36 [00:00<00:00, 52.29it/s]\n",
            "Epoch: 124| Step:  35| Train loss: 4.70280: 100%|██████████| 36/36 [00:00<00:00, 51.65it/s]\n",
            "Epoch: 125| Step:  35| Train loss: 4.56176: 100%|██████████| 36/36 [00:00<00:00, 53.21it/s]\n",
            "Epoch: 126| Step:  35| Train loss: 4.65750: 100%|██████████| 36/36 [00:00<00:00, 52.96it/s]\n",
            "Epoch: 127| Step:  35| Train loss: 4.50937: 100%|██████████| 36/36 [00:00<00:00, 51.76it/s]\n",
            "Epoch: 128| Step:  35| Train loss: 4.66640: 100%|██████████| 36/36 [00:00<00:00, 52.90it/s]\n",
            "Epoch: 129| Step:  35| Train loss: 4.51342: 100%|██████████| 36/36 [00:00<00:00, 53.31it/s]\n",
            "Epoch: 130| Step:  35| Train loss: 4.66203: 100%|██████████| 36/36 [00:00<00:00, 52.96it/s]\n",
            "Epoch: 131| Step:  35| Train loss: 4.57414: 100%|██████████| 36/36 [00:00<00:00, 52.22it/s]\n",
            "Epoch: 132| Step:  35| Train loss: 4.62828: 100%|██████████| 36/36 [00:00<00:00, 52.93it/s]\n",
            "Epoch: 133| Step:  35| Train loss: 4.80511: 100%|██████████| 36/36 [00:00<00:00, 52.80it/s]\n",
            "Epoch: 134| Step:  35| Train loss: 4.61606: 100%|██████████| 36/36 [00:00<00:00, 52.51it/s]\n",
            "Epoch: 135| Step:  35| Train loss: 4.59744: 100%|██████████| 36/36 [00:00<00:00, 53.11it/s]\n",
            "Epoch: 136| Step:  35| Train loss: 4.68829: 100%|██████████| 36/36 [00:00<00:00, 53.17it/s]\n",
            "Epoch: 137| Step:  35| Train loss: 4.50189: 100%|██████████| 36/36 [00:00<00:00, 51.61it/s]\n",
            "Epoch: 138| Step:  35| Train loss: 4.56886: 100%|██████████| 36/36 [00:00<00:00, 53.70it/s]\n",
            "Epoch: 139| Step:  35| Train loss: 4.56331: 100%|██████████| 36/36 [00:00<00:00, 52.05it/s]\n",
            "Epoch: 140| Step:  35| Train loss: 4.57618: 100%|██████████| 36/36 [00:00<00:00, 53.65it/s]\n",
            "Epoch: 141| Step:  35| Train loss: 4.56024: 100%|██████████| 36/36 [00:00<00:00, 52.94it/s]\n",
            "Epoch: 142| Step:  35| Train loss: 4.52235: 100%|██████████| 36/36 [00:00<00:00, 51.67it/s]\n",
            "Epoch: 143| Step:  35| Train loss: 4.67806: 100%|██████████| 36/36 [00:00<00:00, 52.76it/s]\n",
            "Epoch: 144| Step:  35| Train loss: 4.57574: 100%|██████████| 36/36 [00:00<00:00, 53.57it/s]\n",
            "Epoch: 145| Step:  35| Train loss: 4.73688: 100%|██████████| 36/36 [00:00<00:00, 52.78it/s]\n",
            "Epoch: 146| Step:  35| Train loss: 4.60142: 100%|██████████| 36/36 [00:00<00:00, 51.78it/s]\n",
            "Epoch: 147| Step:  35| Train loss: 4.70419: 100%|██████████| 36/36 [00:00<00:00, 52.93it/s]\n",
            "Epoch: 148| Step:  35| Train loss: 4.75498: 100%|██████████| 36/36 [00:00<00:00, 52.84it/s]\n",
            "Epoch: 149| Step:  35| Train loss: 4.57961: 100%|██████████| 36/36 [00:00<00:00, 52.51it/s]\n",
            "Epoch: 150| Step:  35| Train loss: 4.59794: 100%|██████████| 36/36 [00:00<00:00, 53.28it/s]\n",
            "Epoch: 151| Step:  35| Train loss: 4.63460: 100%|██████████| 36/36 [00:00<00:00, 53.90it/s]\n",
            "Epoch: 152| Step:  35| Train loss: 4.53653: 100%|██████████| 36/36 [00:00<00:00, 52.19it/s]\n",
            "Epoch: 153| Step:  35| Train loss: 4.58915: 100%|██████████| 36/36 [00:00<00:00, 52.63it/s]\n",
            "Epoch: 154| Step:  35| Train loss: 4.66954: 100%|██████████| 36/36 [00:00<00:00, 52.18it/s]\n",
            "Epoch: 155| Step:  35| Train loss: 4.69763: 100%|██████████| 36/36 [00:00<00:00, 53.27it/s]\n",
            "Epoch: 156| Step:  35| Train loss: 4.63140: 100%|██████████| 36/36 [00:00<00:00, 53.44it/s]\n",
            "Epoch: 157| Step:  35| Train loss: 4.51802: 100%|██████████| 36/36 [00:00<00:00, 51.04it/s]\n",
            "Epoch: 158| Step:  35| Train loss: 4.61063: 100%|██████████| 36/36 [00:00<00:00, 53.03it/s]\n",
            "Epoch: 159| Step:  35| Train loss: 4.62291: 100%|██████████| 36/36 [00:00<00:00, 52.56it/s]\n",
            "Epoch: 160| Step:  35| Train loss: 4.63706: 100%|██████████| 36/36 [00:00<00:00, 53.00it/s]\n",
            "Epoch: 161| Step:  35| Train loss: 4.60312: 100%|██████████| 36/36 [00:00<00:00, 51.00it/s]\n",
            "Epoch: 162| Step:  35| Train loss: 4.58436: 100%|██████████| 36/36 [00:00<00:00, 52.77it/s]\n",
            "Epoch: 163| Step:  35| Train loss: 4.68284: 100%|██████████| 36/36 [00:00<00:00, 52.55it/s]\n",
            "Epoch: 164| Step:  35| Train loss: 4.65235: 100%|██████████| 36/36 [00:00<00:00, 53.13it/s]\n",
            "Epoch: 165| Step:  35| Train loss: 4.58110: 100%|██████████| 36/36 [00:00<00:00, 52.83it/s]\n",
            "Epoch: 166| Step:  35| Train loss: 4.53468: 100%|██████████| 36/36 [00:00<00:00, 53.14it/s]\n",
            "Epoch: 167| Step:  35| Train loss: 4.65121: 100%|██████████| 36/36 [00:00<00:00, 51.37it/s]\n",
            "Epoch: 168| Step:  35| Train loss: 4.44056: 100%|██████████| 36/36 [00:00<00:00, 53.30it/s]\n",
            "Epoch: 169| Step:  35| Train loss: 4.72819: 100%|██████████| 36/36 [00:00<00:00, 52.63it/s]\n",
            "Epoch: 170| Step:  35| Train loss: 4.58993: 100%|██████████| 36/36 [00:00<00:00, 52.32it/s]\n",
            "Epoch: 171| Step:  35| Train loss: 4.55381: 100%|██████████| 36/36 [00:00<00:00, 50.26it/s]\n",
            "Epoch: 172| Step:  35| Train loss: 4.54729: 100%|██████████| 36/36 [00:00<00:00, 51.69it/s]\n",
            "Epoch: 173| Step:  35| Train loss: 4.50214: 100%|██████████| 36/36 [00:00<00:00, 50.89it/s]\n",
            "Epoch: 174| Step:  35| Train loss: 4.49439: 100%|██████████| 36/36 [00:00<00:00, 51.17it/s]\n",
            "Epoch: 175| Step:  35| Train loss: 4.72456: 100%|██████████| 36/36 [00:00<00:00, 52.08it/s]\n",
            "Epoch: 176| Step:  35| Train loss: 4.50818: 100%|██████████| 36/36 [00:00<00:00, 51.72it/s]\n",
            "Epoch: 177| Step:  35| Train loss: 4.62409: 100%|██████████| 36/36 [00:00<00:00, 52.41it/s]\n",
            "Epoch: 178| Step:  35| Train loss: 4.54649: 100%|██████████| 36/36 [00:00<00:00, 50.23it/s]\n",
            "Epoch: 179| Step:  35| Train loss: 4.69309: 100%|██████████| 36/36 [00:00<00:00, 51.58it/s]\n",
            "Epoch: 180| Step:  35| Train loss: 4.60845: 100%|██████████| 36/36 [00:00<00:00, 50.66it/s]\n",
            "Epoch: 181| Step:  35| Train loss: 4.50301: 100%|██████████| 36/36 [00:00<00:00, 49.45it/s]\n",
            "Epoch: 182| Step:  35| Train loss: 4.56344: 100%|██████████| 36/36 [00:00<00:00, 50.63it/s]\n",
            "Epoch: 183| Step:  35| Train loss: 4.63030: 100%|██████████| 36/36 [00:00<00:00, 50.13it/s]\n",
            "Epoch: 184| Step:  35| Train loss: 4.55823: 100%|██████████| 36/36 [00:00<00:00, 51.64it/s]\n",
            "Epoch: 185| Step:  35| Train loss: 4.55963: 100%|██████████| 36/36 [00:00<00:00, 49.34it/s]\n",
            "Epoch: 186| Step:  35| Train loss: 4.60245: 100%|██████████| 36/36 [00:00<00:00, 50.43it/s]\n",
            "Epoch: 187| Step:  35| Train loss: 4.42355: 100%|██████████| 36/36 [00:00<00:00, 53.71it/s]\n",
            "Epoch: 188| Step:  35| Train loss: 4.52744: 100%|██████████| 36/36 [00:00<00:00, 52.94it/s]\n",
            "Epoch: 189| Step:  35| Train loss: 4.72146: 100%|██████████| 36/36 [00:00<00:00, 52.47it/s]\n",
            "Epoch: 190| Step:  35| Train loss: 4.54321: 100%|██████████| 36/36 [00:00<00:00, 50.43it/s]\n",
            "Epoch: 191| Step:  35| Train loss: 4.49630: 100%|██████████| 36/36 [00:00<00:00, 52.39it/s]\n",
            "Epoch: 192| Step:  35| Train loss: 4.53302: 100%|██████████| 36/36 [00:00<00:00, 50.74it/s]\n",
            "Epoch: 193| Step:  35| Train loss: 4.47217: 100%|██████████| 36/36 [00:00<00:00, 51.25it/s]\n",
            "Epoch: 194| Step:  35| Train loss: 4.58967: 100%|██████████| 36/36 [00:00<00:00, 50.08it/s]\n",
            "Epoch: 195| Step:  35| Train loss: 4.52785: 100%|██████████| 36/36 [00:00<00:00, 49.98it/s]\n",
            "Epoch: 196| Step:  35| Train loss: 4.56348: 100%|██████████| 36/36 [00:00<00:00, 51.71it/s]\n",
            "Epoch: 197| Step:  35| Train loss: 4.67657: 100%|██████████| 36/36 [00:00<00:00, 51.56it/s]\n",
            "Epoch: 198| Step:  35| Train loss: 4.57990: 100%|██████████| 36/36 [00:00<00:00, 51.45it/s]\n",
            "Epoch: 199| Step:  35| Train loss: 4.57803: 100%|██████████| 36/36 [00:00<00:00, 53.15it/s]\n",
            "Epoch: 200| Step:  35| Train loss: 4.40902: 100%|██████████| 36/36 [00:00<00:00, 50.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "NDCG = 0.0 # NDCG@10\n",
        "HIT = 0.0 # HIT@10\n",
        "\n",
        "a = []\n",
        "for u in range(num_user):\n",
        "    if u % 1000 == 0:\n",
        "        print(u)\n",
        "    seq = (user_train[u] + user_valid[u] + [num_item + 1])[-max_len:] \n",
        "    rated = set(user_train[u] + user_valid[u])\n",
        "    \n",
        "    item_idx = [i for i in range(num_item) if i not in rated]\n",
        "    with torch.no_grad():\n",
        "        predictions = - model(np.array([seq]))\n",
        "        predictions = predictions[0][-1][item_idx] # sampling\n",
        "        for i in range(10):\n",
        "            rank = predictions.argsort()[i].item()\n",
        "            a.append([check_user[u],check_item[item_idx[rank]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySUaL4FPAaIO",
        "outputId": "9439f237-0732-40d0-eb2b-5683d16f0b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(a,columns=['user', 'item'])\n",
        "df.to_csv('result_bert.csv', index=False)"
      ],
      "metadata": {
        "id": "JyGGhGIzBRLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과 확인"
      ],
      "metadata": {
        "id": "t6sPX1n4alom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.read_csv('result_bert.csv')"
      ],
      "metadata": {
        "id": "lPvHra5wakq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "survey = result[result['user'] > 4531]"
      ],
      "metadata": {
        "id": "LKNravIia1ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "survey.to_csv(\"origin.csv\")"
      ],
      "metadata": {
        "id": "sB-AHEDWbXUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}