{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzJgxsrewLBy",
        "outputId": "aaa47864-f53f-4e0f-a4b1-9504a58aa764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-box\n",
            "  Downloading python_box-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwQWSIGTsm8f",
        "outputId": "8362454e-f419-4a67-e2b7-2f16cee95e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules & Hyper-parameters"
      ],
      "metadata": {
        "id": "L_Hu0jytxT4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug-br-pPu9vZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlm1UrKvoC_O"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/[22-2]DSL_Modeling\" , # 데이터 경로\n",
        "\n",
        "    'max_len' : 50,\n",
        "    'hidden_units' : 50, # Embedding size\n",
        "    'num_heads' : 1, # Multi-head layer 의 수 (병렬 처리)\n",
        "    'num_layers': 2, # block의 개수 (encoder layer의 개수)\n",
        "    'dropout_rate' : 0.5, # dropout 비율\n",
        "    'lr' : 0.001,\n",
        "    'batch_size' : 128,\n",
        "    'num_epochs' : 200,\n",
        "    'num_workers' : 2,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDxy0fJu9vf"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W64BYWl0u9vg"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'dacon_menu_hr.csv'))\n",
        "        \n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('MenuID')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('SessionID')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['MenuID'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['SessionID'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따라 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "        \n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-1]\n",
        "            user_valid[user] = [users[user][-1]] # 마지막 아이템을 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "    \n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IldCGmY8u9vh"
      },
      "outputs": [],
      "source": [
        "class SASRecDataSet(Dataset):\n",
        "    def __init__(self, user_train, max_len, num_user, num_item):\n",
        "        self.user_train = user_train\n",
        "        self.max_len = max_len\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self._all_items = set([i for i in range(1, self.num_item + 1)])\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, user): \n",
        "        \n",
        "        user_seq = self.user_train[user]\n",
        "        user_seq_len = len(user_seq)\n",
        "\n",
        "        seq = user_seq[-(user_seq_len) : -1]\n",
        "        seq = seq[-self.max_len :]\n",
        "\n",
        "        pos = user_seq[-(user_seq_len - 1) : ]\n",
        "        pos = pos[-self.max_len :]\n",
        "        \n",
        "        neg = random.sample(list(self._all_items - set(user_seq)), len(pos))\n",
        "        \n",
        "        seq = [0] * (self.max_len - len(seq)) + seq\n",
        "        pos = [0] * (self.max_len - len(pos)) + pos\n",
        "        neg = [0] * (self.max_len - len(neg)) + neg\n",
        "        \n",
        "        return np.array(seq, dtype=np.int32), np.array(pos, dtype=np.int32), np.array(neg, dtype=np.int32)\n",
        "\n",
        "    def random_neg_sampling(self, rated_item : list, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(rated_item)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtf1I824nx5V"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        \"\"\"\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
        "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n",
        "        return output, attn_dist\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # head의 수\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        \"\"\"\n",
        "        enc : (batch_size, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        \n",
        "        \"\"\"\n",
        "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합침\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
        "        return output, attn_dist\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(SASRecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        \"\"\"\n",
        "        input_enc : (batch_size, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        \"\"\"\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist\n"
      ],
      "metadata": {
        "id": "YHtV72JbFX7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SASRec"
      ],
      "metadata": {
        "id": "jUVphUfAxfcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SASRec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
        "        super(SASRec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers \n",
        "        self.device = device\n",
        "        \n",
        "        self.item_emb = nn.Embedding(num_item + 1, hidden_units, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "    \n",
        "    def feats(self, log_seqs):\n",
        "        \"\"\"\n",
        "        log_seqs : (batch_size, max_len)\n",
        "\n",
        "        ex)\n",
        "        log_seqs = [\n",
        "                [1, 2, 3, 4, 5],\n",
        "                [0, 0, 0, 1, 2],\n",
        "                [0, 0, 1, 2, 3]\n",
        "        ]\n",
        "        \n",
        "        \"\"\"\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1]) # (batch_size, max_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs)) # LayerNorm\n",
        "\n",
        "        # masking \n",
        "        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_len)\n",
        "        mask_time = (1 - torch.triu(torch.ones((1, 1, seqs.size(1), seqs.size(1))), diagonal=1)).bool() # (batch_size, 1, max_len, max_len)\n",
        "        mask = (mask_pad & mask_time).to(self.device) # (batch_size, 1, max_len, max_len)\n",
        "        for block in self.blocks: \n",
        "            seqs, attn_dist = block(seqs, mask)\n",
        "\n",
        "        # (batch_size, max_len, hidden_units)\n",
        "        return seqs\n",
        "    \n",
        "    def forward(self, log_seqs, pos_seqs, neg_seqs):\n",
        "        \"\"\"\n",
        "        log_seqs : (batch_size, max_len)\n",
        "        pos_seqs : (batch_size, max_len)\n",
        "        neg_seqs : (batch_size, max_len)\n",
        "\n",
        "        ex)\n",
        "        log_seqs = [\n",
        "                [1, 2, 3, 4, 5],\n",
        "                [0, 0, 0, 1, 2],\n",
        "                [0, 0, 1, 2, 3]\n",
        "        ]\n",
        "        \n",
        "        \"\"\"\n",
        "        # 학습에 사용\n",
        "        feats = self.feats(log_seqs)\n",
        "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.device))\n",
        "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.device))\n",
        "        \n",
        "        pos_logits = (feats * pos_embs).sum(dim=-1)\n",
        "        neg_logits = (feats * neg_embs).sum(dim=-1)\n",
        "        return pos_logits, neg_logits\n",
        "    \n",
        "    def predict(self, log_seqs, item_indices):\n",
        "        # evaluation에 사용\n",
        "        final_feats = self.feats(log_seqs)[:, -1, :]\n",
        "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.device))\n",
        "        logits = item_embs.matmul(final_feats.unsqueeze(-1)).squeeze(-1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "p9cjyo7ZxeqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk-bL5p4nx5W"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, pos, neg in data_loader:\n",
        "        pos_logits, neg_logits = model(seq.cpu().numpy(), pos.cpu().numpy(), neg.cpu().numpy())\n",
        "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=device), torch.zeros(neg_logits.shape, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        indices = np.where(pos != 0)\n",
        "        loss = criterion(pos_logits[indices], pos_labels[indices])\n",
        "        loss += criterion(neg_logits[indices], neg_labels[indices])\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, max_len, sasrec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = user_train[user][-max_len:]\n",
        "        rated = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + sasrec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n",
        "        with torch.no_grad():\n",
        "            predictions = -model.predict(np.array([seq]), np.array(items))\n",
        "            predictions = predictions[0]\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: #Top10\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ],
      "metadata": {
        "id": "9YGrj0ztii9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozBvgarCnx5W"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config = config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ],
      "metadata": {
        "id": "tb8nwGb-Lufv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sasrec_dataset = SASRecDataSet(\n",
        "    user_train = user_train, \n",
        "    max_len = config.max_len, \n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item, \n",
        "    )"
      ],
      "metadata": {
        "id": "6mkKkbLtYYJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(\n",
        "    sasrec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ],
      "metadata": {
        "id": "hzMIEy4kYY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SASRec(\n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    max_len = config.max_len, \n",
        "    dropout_rate = config.dropout_rate, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ],
      "metadata": {
        "id": "FrphMwGbYaFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ndcg = 0\n",
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            max_len = config.max_len,\n",
        "            sasrec_dataset = sasrec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyNfpRIAYdAC",
        "outputId": "8a4042ff-0ed7-4ee5-a5f5-351760df38fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 5.41836| NDCG@10: 0.06697| HIT@10: 0.13679: 100%|██████████| 1/1 [00:12<00:00, 12.25s/it]\n",
            "Epoch:   2| Train loss: 4.54563| NDCG@10: 0.08579| HIT@10: 0.17530: 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
            "Epoch:   3| Train loss: 3.80100| NDCG@10: 0.09274| HIT@10: 0.19743: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch:   4| Train loss: 3.26549| NDCG@10: 0.10763| HIT@10: 0.22266: 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "Epoch:   5| Train loss: 2.80388| NDCG@10: 0.12373| HIT@10: 0.25409: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:   6| Train loss: 2.44656| NDCG@10: 0.14089| HIT@10: 0.28442: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:   7| Train loss: 2.17136| NDCG@10: 0.15878| HIT@10: 0.31983: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:   8| Train loss: 1.96115| NDCG@10: 0.17462| HIT@10: 0.34971: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:   9| Train loss: 1.78583| NDCG@10: 0.19807| HIT@10: 0.38092: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  10| Train loss: 1.65789| NDCG@10: 0.21360| HIT@10: 0.40969: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch:  11| Train loss: 1.54893| NDCG@10: 0.22913| HIT@10: 0.42873: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "Epoch:  12| Train loss: 1.47148| NDCG@10: 0.24575| HIT@10: 0.45728: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  13| Train loss: 1.39903| NDCG@10: 0.26001| HIT@10: 0.47189: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "Epoch:  14| Train loss: 1.33455| NDCG@10: 0.27027| HIT@10: 0.48052: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch:  15| Train loss: 1.28770| NDCG@10: 0.27621| HIT@10: 0.48982: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "Epoch:  16| Train loss: 1.25736| NDCG@10: 0.28135| HIT@10: 0.50310: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  17| Train loss: 1.22760| NDCG@10: 0.28309| HIT@10: 0.50155: 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "Epoch:  18| Train loss: 1.19728| NDCG@10: 0.29380| HIT@10: 0.51173: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch:  19| Train loss: 1.17238| NDCG@10: 0.29327| HIT@10: 0.51483: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch:  20| Train loss: 1.15244| NDCG@10: 0.29055| HIT@10: 0.51173: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  21| Train loss: 1.13719| NDCG@10: 0.29267| HIT@10: 0.51239: 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "Epoch:  22| Train loss: 1.12171| NDCG@10: 0.29816| HIT@10: 0.52147: 100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "Epoch:  23| Train loss: 1.10843| NDCG@10: 0.29470| HIT@10: 0.51726: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch:  24| Train loss: 1.09429| NDCG@10: 0.30034| HIT@10: 0.52545: 100%|██████████| 1/1 [00:09<00:00,  9.28s/it]\n",
            "Epoch:  25| Train loss: 1.08641| NDCG@10: 0.29738| HIT@10: 0.51948: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  26| Train loss: 1.07309| NDCG@10: 0.29565| HIT@10: 0.52036: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "Epoch:  27| Train loss: 1.06656| NDCG@10: 0.29829| HIT@10: 0.51771: 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "Epoch:  28| Train loss: 1.05983| NDCG@10: 0.29635| HIT@10: 0.52413: 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "Epoch:  29| Train loss: 1.05888| NDCG@10: 0.29712| HIT@10: 0.51903: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "Epoch:  30| Train loss: 1.05112| NDCG@10: 0.29775| HIT@10: 0.51815: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch:  31| Train loss: 1.04379| NDCG@10: 0.29606| HIT@10: 0.52368: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  32| Train loss: 1.03915| NDCG@10: 0.29686| HIT@10: 0.51903: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
            "Epoch:  33| Train loss: 1.03564| NDCG@10: 0.29865| HIT@10: 0.51881: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  34| Train loss: 1.02857| NDCG@10: 0.30073| HIT@10: 0.52058: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n",
            "Epoch:  35| Train loss: 1.02738| NDCG@10: 0.29412| HIT@10: 0.51660: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch:  36| Train loss: 1.02655| NDCG@10: 0.29850| HIT@10: 0.51726: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:  37| Train loss: 1.02069| NDCG@10: 0.30041| HIT@10: 0.51837: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch:  38| Train loss: 1.02497| NDCG@10: 0.29779| HIT@10: 0.51948: 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "Epoch:  39| Train loss: 1.01964| NDCG@10: 0.29931| HIT@10: 0.52036: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:  40| Train loss: 1.01621| NDCG@10: 0.29813| HIT@10: 0.52302: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:  41| Train loss: 1.01580| NDCG@10: 0.29595| HIT@10: 0.52413: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch:  42| Train loss: 1.01571| NDCG@10: 0.30076| HIT@10: 0.52413: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch:  43| Train loss: 1.01685| NDCG@10: 0.29881| HIT@10: 0.52213: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch:  44| Train loss: 1.01416| NDCG@10: 0.29827| HIT@10: 0.52368: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch:  45| Train loss: 1.01037| NDCG@10: 0.30043| HIT@10: 0.52390: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  46| Train loss: 1.01394| NDCG@10: 0.29753| HIT@10: 0.52855: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch:  47| Train loss: 1.00953| NDCG@10: 0.30031| HIT@10: 0.52678: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  48| Train loss: 1.00906| NDCG@10: 0.30018| HIT@10: 0.52280: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  49| Train loss: 1.00788| NDCG@10: 0.29777| HIT@10: 0.52125: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  50| Train loss: 1.00466| NDCG@10: 0.29928| HIT@10: 0.51815: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:  51| Train loss: 1.00433| NDCG@10: 0.30057| HIT@10: 0.52236: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch:  52| Train loss: 1.00377| NDCG@10: 0.30053| HIT@10: 0.52258: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  53| Train loss: 1.00582| NDCG@10: 0.30058| HIT@10: 0.52147: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch:  54| Train loss: 1.00329| NDCG@10: 0.29705| HIT@10: 0.51638: 100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n",
            "Epoch:  55| Train loss: 1.00474| NDCG@10: 0.29814| HIT@10: 0.52169: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  56| Train loss: 1.00502| NDCG@10: 0.30127| HIT@10: 0.52125: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch:  57| Train loss: 1.00336| NDCG@10: 0.30193| HIT@10: 0.52213: 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "Epoch:  58| Train loss: 1.00464| NDCG@10: 0.30072| HIT@10: 0.52767: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "Epoch:  59| Train loss: 1.00193| NDCG@10: 0.30014| HIT@10: 0.52302: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch:  60| Train loss: 1.00062| NDCG@10: 0.30350| HIT@10: 0.52568: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  61| Train loss: 1.00175| NDCG@10: 0.30430| HIT@10: 0.52568: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  62| Train loss: 1.00109| NDCG@10: 0.29895| HIT@10: 0.52457: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  63| Train loss: 1.00160| NDCG@10: 0.30263| HIT@10: 0.52634: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  64| Train loss: 0.99838| NDCG@10: 0.29733| HIT@10: 0.52368: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  65| Train loss: 1.00107| NDCG@10: 0.29835| HIT@10: 0.52169: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch:  66| Train loss: 1.00094| NDCG@10: 0.30212| HIT@10: 0.52435: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "Epoch:  67| Train loss: 0.99882| NDCG@10: 0.30021| HIT@10: 0.52302: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  68| Train loss: 1.00058| NDCG@10: 0.30253| HIT@10: 0.52922: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  69| Train loss: 1.00125| NDCG@10: 0.30252| HIT@10: 0.52058: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch:  70| Train loss: 0.99980| NDCG@10: 0.29817| HIT@10: 0.51726: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  71| Train loss: 1.00058| NDCG@10: 0.29770| HIT@10: 0.52280: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  72| Train loss: 0.99975| NDCG@10: 0.30002| HIT@10: 0.52258: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch:  73| Train loss: 0.99937| NDCG@10: 0.30089| HIT@10: 0.52258: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  74| Train loss: 1.00004| NDCG@10: 0.29858| HIT@10: 0.51881: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch:  75| Train loss: 0.99851| NDCG@10: 0.30174| HIT@10: 0.52745: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch:  76| Train loss: 0.99686| NDCG@10: 0.29866| HIT@10: 0.52125: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  77| Train loss: 0.99527| NDCG@10: 0.29783| HIT@10: 0.52058: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  78| Train loss: 0.99608| NDCG@10: 0.29662| HIT@10: 0.52147: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  79| Train loss: 0.99564| NDCG@10: 0.30095| HIT@10: 0.52501: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "Epoch:  80| Train loss: 0.99934| NDCG@10: 0.29990| HIT@10: 0.52545: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch:  81| Train loss: 0.99682| NDCG@10: 0.30069| HIT@10: 0.52302: 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "Epoch:  82| Train loss: 0.99619| NDCG@10: 0.29734| HIT@10: 0.52058: 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "Epoch:  83| Train loss: 0.99819| NDCG@10: 0.30304| HIT@10: 0.52745: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch:  84| Train loss: 0.99782| NDCG@10: 0.30281| HIT@10: 0.52390: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch:  85| Train loss: 0.99560| NDCG@10: 0.29958| HIT@10: 0.51926: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  86| Train loss: 0.99851| NDCG@10: 0.30112| HIT@10: 0.51948: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch:  87| Train loss: 0.99277| NDCG@10: 0.30016| HIT@10: 0.52789: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  88| Train loss: 0.99626| NDCG@10: 0.30176| HIT@10: 0.51926: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch:  89| Train loss: 0.99396| NDCG@10: 0.30145| HIT@10: 0.52169: 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "Epoch:  90| Train loss: 0.99716| NDCG@10: 0.30040| HIT@10: 0.52457: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch:  91| Train loss: 0.99359| NDCG@10: 0.30129| HIT@10: 0.52258: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "Epoch:  92| Train loss: 0.99441| NDCG@10: 0.30112| HIT@10: 0.52457: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch:  93| Train loss: 0.99283| NDCG@10: 0.29999| HIT@10: 0.52413: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch:  94| Train loss: 0.99312| NDCG@10: 0.30176| HIT@10: 0.52722: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch:  95| Train loss: 0.99323| NDCG@10: 0.29965| HIT@10: 0.52103: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "Epoch:  96| Train loss: 0.99592| NDCG@10: 0.30265| HIT@10: 0.52302: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch:  97| Train loss: 0.99372| NDCG@10: 0.29784| HIT@10: 0.52346: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch:  98| Train loss: 0.99555| NDCG@10: 0.29929| HIT@10: 0.52390: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n",
            "Epoch:  99| Train loss: 0.99258| NDCG@10: 0.29976| HIT@10: 0.52081: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 100| Train loss: 0.99382| NDCG@10: 0.30074| HIT@10: 0.52501: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch: 101| Train loss: 0.99286| NDCG@10: 0.29883| HIT@10: 0.51749: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 102| Train loss: 0.99368| NDCG@10: 0.29871| HIT@10: 0.51837: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 103| Train loss: 0.99441| NDCG@10: 0.29721| HIT@10: 0.51262: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n",
            "Epoch: 104| Train loss: 0.99452| NDCG@10: 0.29918| HIT@10: 0.52280: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 105| Train loss: 0.99366| NDCG@10: 0.29869| HIT@10: 0.52811: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch: 106| Train loss: 0.98861| NDCG@10: 0.29479| HIT@10: 0.51881: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 107| Train loss: 0.99012| NDCG@10: 0.29998| HIT@10: 0.51704: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 108| Train loss: 0.99062| NDCG@10: 0.30014| HIT@10: 0.52236: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 109| Train loss: 0.99202| NDCG@10: 0.29650| HIT@10: 0.52058: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 110| Train loss: 0.99024| NDCG@10: 0.29674| HIT@10: 0.52457: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 111| Train loss: 0.99177| NDCG@10: 0.29801| HIT@10: 0.52036: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 112| Train loss: 0.98868| NDCG@10: 0.30178| HIT@10: 0.52280: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 113| Train loss: 0.98817| NDCG@10: 0.29933| HIT@10: 0.51549: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 114| Train loss: 0.99010| NDCG@10: 0.29832| HIT@10: 0.52125: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 115| Train loss: 0.98933| NDCG@10: 0.29839| HIT@10: 0.52236: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch: 116| Train loss: 0.98801| NDCG@10: 0.29592| HIT@10: 0.52036: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "Epoch: 117| Train loss: 0.98402| NDCG@10: 0.29948| HIT@10: 0.52855: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 118| Train loss: 0.98458| NDCG@10: 0.29883| HIT@10: 0.52191: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "Epoch: 119| Train loss: 0.98653| NDCG@10: 0.29572| HIT@10: 0.52169: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
            "Epoch: 120| Train loss: 0.98116| NDCG@10: 0.29684| HIT@10: 0.52147: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 121| Train loss: 0.98301| NDCG@10: 0.29446| HIT@10: 0.51726: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 122| Train loss: 0.98048| NDCG@10: 0.29606| HIT@10: 0.52479: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "Epoch: 123| Train loss: 0.97955| NDCG@10: 0.29878| HIT@10: 0.52745: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch: 124| Train loss: 0.97849| NDCG@10: 0.29924| HIT@10: 0.52346: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 125| Train loss: 0.97831| NDCG@10: 0.30181| HIT@10: 0.52545: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 126| Train loss: 0.97819| NDCG@10: 0.29922| HIT@10: 0.52324: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch: 127| Train loss: 0.97304| NDCG@10: 0.30011| HIT@10: 0.52191: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch: 128| Train loss: 0.97006| NDCG@10: 0.30199| HIT@10: 0.52789: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 129| Train loss: 0.96970| NDCG@10: 0.30014| HIT@10: 0.53165: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch: 130| Train loss: 0.96654| NDCG@10: 0.30189| HIT@10: 0.53254: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 131| Train loss: 0.96796| NDCG@10: 0.30036| HIT@10: 0.52722: 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "Epoch: 132| Train loss: 0.96265| NDCG@10: 0.30865| HIT@10: 0.53586: 100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n",
            "Epoch: 133| Train loss: 0.96112| NDCG@10: 0.30307| HIT@10: 0.53209: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch: 134| Train loss: 0.95911| NDCG@10: 0.31171| HIT@10: 0.54471: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 135| Train loss: 0.95580| NDCG@10: 0.30512| HIT@10: 0.53652: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "Epoch: 136| Train loss: 0.95593| NDCG@10: 0.31067| HIT@10: 0.54648: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 137| Train loss: 0.95098| NDCG@10: 0.31188| HIT@10: 0.54714: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "Epoch: 138| Train loss: 0.94702| NDCG@10: 0.31023| HIT@10: 0.54205: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 139| Train loss: 0.94406| NDCG@10: 0.31270| HIT@10: 0.54869: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "Epoch: 140| Train loss: 0.93822| NDCG@10: 0.31468| HIT@10: 0.55268: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 141| Train loss: 0.93027| NDCG@10: 0.31527| HIT@10: 0.55378: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "Epoch: 142| Train loss: 0.92783| NDCG@10: 0.31915| HIT@10: 0.56087: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 143| Train loss: 0.92370| NDCG@10: 0.32598| HIT@10: 0.56419: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 144| Train loss: 0.92105| NDCG@10: 0.32249| HIT@10: 0.55777: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch: 145| Train loss: 0.91494| NDCG@10: 0.32985| HIT@10: 0.57304: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "Epoch: 146| Train loss: 0.91304| NDCG@10: 0.33018| HIT@10: 0.57260: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 147| Train loss: 0.90538| NDCG@10: 0.32904| HIT@10: 0.58145: 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "Epoch: 148| Train loss: 0.90443| NDCG@10: 0.33022| HIT@10: 0.57592: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 149| Train loss: 0.89734| NDCG@10: 0.33135| HIT@10: 0.57592: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch: 150| Train loss: 0.89558| NDCG@10: 0.33393| HIT@10: 0.57680: 100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "Epoch: 151| Train loss: 0.88782| NDCG@10: 0.33243| HIT@10: 0.58101: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch: 152| Train loss: 0.88558| NDCG@10: 0.33425| HIT@10: 0.58389: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch: 153| Train loss: 0.88142| NDCG@10: 0.33296| HIT@10: 0.58256: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "Epoch: 154| Train loss: 0.87195| NDCG@10: 0.33341| HIT@10: 0.58212: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 155| Train loss: 0.86593| NDCG@10: 0.33370| HIT@10: 0.58588: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch: 156| Train loss: 0.86622| NDCG@10: 0.33727| HIT@10: 0.58876: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 157| Train loss: 0.85948| NDCG@10: 0.33572| HIT@10: 0.58012: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch: 158| Train loss: 0.85357| NDCG@10: 0.34048| HIT@10: 0.59650: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 159| Train loss: 0.84924| NDCG@10: 0.33837| HIT@10: 0.58721: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 160| Train loss: 0.84695| NDCG@10: 0.33586| HIT@10: 0.58721: 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "Epoch: 161| Train loss: 0.84444| NDCG@10: 0.33982| HIT@10: 0.58809: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "Epoch: 162| Train loss: 0.83543| NDCG@10: 0.33980| HIT@10: 0.59097: 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "Epoch: 163| Train loss: 0.83434| NDCG@10: 0.33610| HIT@10: 0.58743: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "Epoch: 164| Train loss: 0.82892| NDCG@10: 0.33291| HIT@10: 0.58322: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "Epoch: 165| Train loss: 0.82361| NDCG@10: 0.33762| HIT@10: 0.59274: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch: 166| Train loss: 0.81700| NDCG@10: 0.33268| HIT@10: 0.58765: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "Epoch: 167| Train loss: 0.81428| NDCG@10: 0.33097| HIT@10: 0.58300: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 168| Train loss: 0.80930| NDCG@10: 0.32475| HIT@10: 0.58300: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 169| Train loss: 0.80684| NDCG@10: 0.33211| HIT@10: 0.58455: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 170| Train loss: 0.80029| NDCG@10: 0.32838| HIT@10: 0.58853: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 171| Train loss: 0.79813| NDCG@10: 0.32901| HIT@10: 0.58544: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 172| Train loss: 0.79121| NDCG@10: 0.32719| HIT@10: 0.58101: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch: 173| Train loss: 0.79144| NDCG@10: 0.32099| HIT@10: 0.57835: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "Epoch: 174| Train loss: 0.78146| NDCG@10: 0.32340| HIT@10: 0.57703: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch: 175| Train loss: 0.77863| NDCG@10: 0.32929| HIT@10: 0.58322: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 176| Train loss: 0.77741| NDCG@10: 0.32606| HIT@10: 0.57990: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 177| Train loss: 0.77319| NDCG@10: 0.32793| HIT@10: 0.58079: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch: 178| Train loss: 0.76881| NDCG@10: 0.32837| HIT@10: 0.58455: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "Epoch: 179| Train loss: 0.76866| NDCG@10: 0.33126| HIT@10: 0.58367: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch: 180| Train loss: 0.76716| NDCG@10: 0.32260| HIT@10: 0.57658: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch: 181| Train loss: 0.76145| NDCG@10: 0.32338| HIT@10: 0.57658: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "Epoch: 182| Train loss: 0.75158| NDCG@10: 0.31897| HIT@10: 0.57105: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "Epoch: 183| Train loss: 0.75040| NDCG@10: 0.32331| HIT@10: 0.57835: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 184| Train loss: 0.75072| NDCG@10: 0.32120| HIT@10: 0.57459: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "Epoch: 185| Train loss: 0.74637| NDCG@10: 0.32112| HIT@10: 0.57326: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "Epoch: 186| Train loss: 0.74343| NDCG@10: 0.32064| HIT@10: 0.56950: 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "Epoch: 187| Train loss: 0.74156| NDCG@10: 0.32228| HIT@10: 0.57171: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 188| Train loss: 0.73948| NDCG@10: 0.31958| HIT@10: 0.56839: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "Epoch: 189| Train loss: 0.73294| NDCG@10: 0.31277| HIT@10: 0.56087: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "Epoch: 190| Train loss: 0.73279| NDCG@10: 0.31586| HIT@10: 0.56884: 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "Epoch: 191| Train loss: 0.72478| NDCG@10: 0.31807| HIT@10: 0.57459: 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "Epoch: 192| Train loss: 0.72445| NDCG@10: 0.32030| HIT@10: 0.57216: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch: 193| Train loss: 0.72660| NDCG@10: 0.32124| HIT@10: 0.57083: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "Epoch: 194| Train loss: 0.72140| NDCG@10: 0.31625| HIT@10: 0.56707: 100%|██████████| 1/1 [00:09<00:00,  9.37s/it]\n",
            "Epoch: 195| Train loss: 0.71629| NDCG@10: 0.31484| HIT@10: 0.56596: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "Epoch: 196| Train loss: 0.71756| NDCG@10: 0.30434| HIT@10: 0.55445: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "Epoch: 197| Train loss: 0.71440| NDCG@10: 0.30673| HIT@10: 0.55644: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch: 198| Train loss: 0.70689| NDCG@10: 0.31205| HIT@10: 0.56529: 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "Epoch: 199| Train loss: 0.70491| NDCG@10: 0.31432| HIT@10: 0.56441: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "Epoch: 200| Train loss: 0.70194| NDCG@10: 0.30395| HIT@10: 0.55644: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "QR_pOciFYtT7",
        "outputId": "48727b56-0aa7-4619-9597-b92a1a1d73f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}